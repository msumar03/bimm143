---
title: "Class 7: Machine Learning"
author: "Mehek (PID: A16868697)"
format: gfm
---

Today we are going to learn how to apply different machine learning methods, beginning with clustering:

The goals here is to find groups/clusters in your input data. 

First I will make up some data with clear groups. For this I will use the `rnorm()` function: 

```{r}
rnorm(10)
```

```{r}
hist(rnorm(10000, mean=3))
```

```{r}
n <-10000
x <- c(rnorm(n,-3), rnorm(n,+3))
hist(x)       
```

```{r}
n <-30
x <- c(rnorm(n,-3), rnorm(n,+3))
y <- rev(x)

z<- cbind(x,y)
head(z)
```

```{r}
plot(z)
```

Use the `kmeans()` function setting k to 2 and nstart=20

Inspect/print the results

>Q. How many points are in each cluster?

>Q. What 'component' of your result object details 
  -cluster size?
  -cluster assignment/member
  -cluster center?
>Q. Plot x colored by the k means closter assignment and add cluster centers as blue points 

```{r}
km <- kmeans(z, centers=2)
km
```

Results in kmeans object `km` 
```{r}
attributes(km)
```

cluster size?
```{r}
km$size
```

cluster assignment/membership?
```{r}
km$cluster
```

cluster center?
```{r}
km$centers
```

>Q. Plot x colored by the k means closter assignment and add cluster centers as blue points 

```{r}
plot(z, col="red")
```
R will re-cycle the shorter color vector to be the same length as the longer (number of data points) in z

```{r}
plot(z, col=c("red", "blue"))
```

```{r}
plot(z, col=c(1,2))
```
```{r}
plot(z, col=km$cluster)
```
We can use the `points()` function to add new points to an existing plot...like the cluster centers. 

```{r}
plot(z, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=3)
```

>Q. Can you run kmeans and ask for 4 clusters please and plot the results like we have done above?

```{r}
km1 <- kmeans(z, centers=4)
km1
plot(z, col=km1$cluster)
points(km1$centers, col="blue", pch=15, cex=1.5)
```


##Hierarchal Clustering 

Let's take our same made-up data `z` and see how hierarchal clustering works.

First we need a distance matrix of our data to be clustered. 
```{r}
d<- dist(z)
hc<- hclust(d)
hc
```
```{r}
plot(hc)
abline(h=8, col="red")
```
I can get my cluster membership vector by "cutting the tree" with the `cutree()` function like so:
```{r}
grps<- cutree(hc, h=8)
grps
```

Can you plot `z` colored by our hclust results:
```{r}
plot(z, col=grps)
```

## PCA of UK food data

Read data from the UK on food consumption in different parts of the UK

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url,row.names=1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

A so-called "Pairs" plot can be useful for small datasets like this one

```{r}
pairs(x, col=rainbow(10), pch=16)
```

It is hard to see structure and trends in even this small data-set. How will we ever do this when we have big datasets with 1,000s or 10s of thousands of this we are measuring...


### PCA to the rescue 

Let's see how PCA deals with this dataset. So main function in base R to do PCA is called `prcomp()`



```{r}
pca<- prcomp( t(x))
summary(pca)
```

Let's see what is inside this `pca` object that we created from running `prcomp()`
```{r}
attributes(pca)
```


```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2], 
     col=c("black", "red", "blue", "darkgreen"), pch=16,
     xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

